{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "650ff753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# 0. Import Libraries and Load Data\n",
    "# ====================================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "## For Dynamic Pathing ###\n",
    "from pathlib import Path\n",
    "current_dir = Path.cwd()\n",
    "base_dir = current_dir.parent\n",
    "data_path = base_dir / 'data' / 'raw' / 'water_quality.csv'\n",
    "\n",
    "# Load the dataset\n",
    "try:\n",
    "    df = pd.read_csv(data_path)\n",
    "    print(\"‚úÖ Dataset loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå Error: 'data/raw/water_quality.csv' not found.\")\n",
    "    df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415db699",
   "metadata": {},
   "source": [
    "## Phase 3: Optimization & Serialization\n",
    "---\n",
    "In this phase, we will take the best-performing model from our baseline evaluation, **Gradient Boosting**, and optimize it using hyperparameter tuning. We will then save the final, tuned model for future use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea37989d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# 1. Data Preparation (Repeating the steps from Phase 2)\n",
    "# ====================================================================\n",
    "# This section repeats the exact data preparation steps to ensure consistency.\n",
    "\n",
    "if not df.empty:\n",
    "    # 1.1. Feature and Target Selection\n",
    "    features = ['EC', 'Cl', 'TDS', 'Na']\n",
    "    target = 'Water Quality Classification'\n",
    "    X = df[features]\n",
    "    y = df[target]\n",
    "\n",
    "    # 1.2. Encode Target Variable\n",
    "    le = LabelEncoder()\n",
    "    y_encoded = le.fit_transform(y)\n",
    "    class_names = le.classes_\n",
    "\n",
    "    # 1.3. Stratified Train-Test Split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y_encoded,\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        stratify=y_encoded\n",
    "    )\n",
    "\n",
    "    # 1.4. Feature Scaling\n",
    "    scaler = MinMaxScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    print(\"‚úÖ Data preparation complete.\")\n",
    "    print(\"Training data shape:\", X_train_scaled.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fdb353",
   "metadata": {},
   "source": [
    "## 2. Hyperparameter Tuning with RandomizedSearchCV\n",
    "---\n",
    "We use `RandomizedSearchCV` because it's much more efficient than searching every possible combination (`GridSearchCV`). It samples a fixed number of parameter settings from the specified distributions.\n",
    "\n",
    "We will optimize for the **Macro-F1 score**, as it's a robust metric for imbalanced classification problems.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c399bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting hyperparameter tuning...\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "‚úÖ Tuning complete.\n"
     ]
    }
   ],
   "source": [
    "# ====================================================================\n",
    "# 2.1. Define the Hyperparameter Grid\n",
    "# ====================================================================\n",
    "\n",
    "# Define the grid of hyperparameters to search\n",
    "param_distributions = {\n",
    "    \"n_estimators\": [100, 200, 300, 500],\n",
    "    \"learning_rate\": [0.01, 0.05, 0.1, 0.2],\n",
    "    \"max_depth\": [3, 5, 7, 10],\n",
    "    \"subsample\": [0.7, 0.8, 0.9, 1.0],\n",
    "    \"max_features\": ['sqrt', 'log2']\n",
    "}\n",
    "\n",
    "# ====================================================================\n",
    "# 2.2. Set up and Run RandomizedSearchCV\n",
    "# ====================================================================\n",
    "if not df.empty:\n",
    "    # Initialize the base model\n",
    "    gb_model = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "    # Set up RandomizedSearchCV\n",
    "    # n_iter=50 means it will try 50 different combinations of parameters.\n",
    "    # cv=5 means 5-fold cross-validation.\n",
    "    # n_jobs=-1 uses all available CPU cores to speed up the process.\n",
    "    random_search = RandomizedSearchCV(\n",
    "        estimator=gb_model,\n",
    "        param_distributions=param_distributions,\n",
    "        n_iter=50,\n",
    "        cv=5,\n",
    "        scoring='f1_macro',\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "        verbose=2 # Shows progress\n",
    "    )\n",
    "\n",
    "    print(\"üöÄ Starting hyperparameter tuning...\")\n",
    "    # Fit the random search to the data\n",
    "    random_search.fit(X_train_scaled, y_train)\n",
    "    print(\"‚úÖ Tuning complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "29e505ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# 3. Analyze Tuning Results\n",
    "# ====================================================================\n",
    "if 'random_search' in locals():\n",
    "    print(\"\\nüèÜ Best Hyperparameters Found:\")\n",
    "    print(random_search.best_params_)\n",
    "\n",
    "    print(f\"\\n‚≠ê Best Macro-F1 Score from Cross-Validation: {random_search.best_score_:.4f}\")\n",
    "    \n",
    "    # The best model found by the search\n",
    "    best_model = random_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f0b785",
   "metadata": {},
   "source": [
    "## 4. Final Evaluation of the Optimized Model\n",
    "---\n",
    "Now we evaluate the `best_model` (the one with the optimal hyperparameters) on the held-out test set to see how much it improved over the baseline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c216ff5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# 4.1. Evaluate the Tuned Model on the Test Set\n",
    "# ====================================================================\n",
    "if 'best_model' in locals():\n",
    "    y_pred_tuned = best_model.predict(X_test_scaled)\n",
    "\n",
    "    # Evaluate the tuned model\n",
    "    accuracy_tuned = accuracy_score(y_test, y_pred_tuned)\n",
    "    report_tuned = classification_report(y_test, y_pred_tuned, target_names=class_names)\n",
    "\n",
    "    print(\"‚úÖ Results for Tuned Gradient Boosting Model:\")\n",
    "    print(f\"Accuracy: {accuracy_tuned:.4f}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(report_tuned)\n",
    "\n",
    "    # Visualize the final Confusion Matrix\n",
    "    cm_tuned = confusion_matrix(y_test, y_pred_tuned)\n",
    "    disp_tuned = ConfusionMatrixDisplay(confusion_matrix=cm_tuned, display_labels=class_names)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    disp_tuned.plot(ax=ax, cmap='Greens', xticks_rotation='vertical')\n",
    "    ax.set_title('Confusion Matrix for Tuned Gradient Boosting Model')\n",
    "    \n",
    "    # Save the figure\n",
    "    cm_path_tuned = 'reports/figures/confusion_matrix_Tuned_Gradient_Boosting.png'\n",
    "    plt.savefig(cm_path_tuned, bbox_inches='tight')\n",
    "    print(f\"\\n‚úÖ Final confusion matrix saved to {cm_path_tuned}\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89796d4d",
   "metadata": {},
   "source": [
    "## 5. Model Serialization\n",
    "---\n",
    "The final step is to save our work. We serialize and save two crucial components:\n",
    "1.  **The Tuned Model**: The `best_model` object itself, ready for making future predictions.\n",
    "2.  **The Scaler**: The `MinMaxScaler` object. This is **critical** because any new data must be scaled in exactly the same way as the training data before being fed to the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "13d47ce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Final model saved to: models/final_gradient_boosting_model.pkl\n",
      "üíæ Scaler saved to: models/min_max_scaler.pkl\n"
     ]
    }
   ],
   "source": [
    "# ====================================================================\n",
    "# 5.1. Save the Model and Scaler to Disk\n",
    "# ====================================================================\n",
    "if 'best_model' in locals() and 'scaler' in locals():\n",
    "    # Create the models directory if it doesn't exist\n",
    "    if not os.path.exists('models'):\n",
    "        os.makedirs('models')\n",
    "\n",
    "    # Define file paths\n",
    "    model_path = 'models/final_gradient_boosting_model.pkl'\n",
    "    scaler_path = 'models/min_max_scaler.pkl'\n",
    "\n",
    "    # Save the model\n",
    "    with open(model_path, 'wb') as f:\n",
    "        pickle.dump(best_model, f)\n",
    "    print(f\"üíæ Final model saved to: {model_path}\")\n",
    "\n",
    "    # Save the scaler\n",
    "    with open(scaler_path, 'wb') as f:\n",
    "        pickle.dump(scaler, f)\n",
    "    print(f\"üíæ Scaler saved to: {scaler_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
