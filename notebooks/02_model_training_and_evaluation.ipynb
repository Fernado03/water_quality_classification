{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d142798f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# 0. Import Libraries and Load Data\n",
    "# ====================================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "## For Dynamic Pathing ###\n",
    "from pathlib import Path\n",
    "current_dir = Path.cwd()\n",
    "base_dir = current_dir.parent\n",
    "data_path = base_dir / 'data' / 'raw' / 'water_quality.csv'\n",
    "\n",
    "# Load the dataset\n",
    "try:\n",
    "    df = pd.read_csv(data_path)\n",
    "    print(\"‚úÖ Dataset loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå Error: 'data/raw/water_quality.csv' not found.\")\n",
    "    df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57272b9b",
   "metadata": {},
   "source": [
    "## Phase 2: Data Preparation & Modeling\n",
    "---\n",
    "This notebook covers the data preprocessing and baseline model training steps. We will:\n",
    "1. Select the features identified in the EDA phase.\n",
    "2. Encode the target variable.\n",
    "3. Split the data into stratified training and testing sets.\n",
    "4. Apply Min-Max scaling to the features.\n",
    "5. Train and evaluate several baseline classification models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63530fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# 1. Feature and Target Selection\n",
    "# ====================================================================\n",
    "if not df.empty:\n",
    "    # Based on the correlation analysis from Phase 1\n",
    "    features = ['EC', 'Cl', 'TDS', 'Na']\n",
    "    target = 'Water Quality Classification'\n",
    "\n",
    "    X = df[features]\n",
    "    y = df[target]\n",
    "\n",
    "    print(\"Selected Features (X):\")\n",
    "    print(X.head())\n",
    "    print(\"\\nTarget Variable (y):\")\n",
    "    print(y.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d6e409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# 2. Encode the Categorical Target Variable\n",
    "# ====================================================================\n",
    "if not df.empty:\n",
    "    le = LabelEncoder()\n",
    "    y_encoded = le.fit_transform(y)\n",
    "\n",
    "    print(\"\\nOriginal Target Labels:\", le.classes_)\n",
    "    print(\"Encoded Target Labels:\", np.unique(y_encoded))\n",
    "    # Storing class names for later use in plots\n",
    "    class_names = le.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0400da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# 3. Stratified Train-Test Split\n",
    "# ====================================================================\n",
    "# We use an 80/20 split and stratify by the encoded target variable `y_encoded`\n",
    "# to ensure the class distribution is the same in both train and test sets.\n",
    "if not df.empty:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y_encoded,\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        stratify=y_encoded\n",
    "    )\n",
    "\n",
    "    print(\"Shape of training data (X_train):\", X_train.shape)\n",
    "    print(\"Shape of testing data (X_test):\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b1575e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# 4. Feature Scaling (Min-Max Scaler)\n",
    "# ====================================================================\n",
    "if not df.empty:\n",
    "    scaler = MinMaxScaler()\n",
    "\n",
    "    # Fit the scaler on the training data and transform it\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "    # Use the same fitted scaler to transform the test data\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Convert scaled arrays back to DataFrames for clarity\n",
    "    X_train_scaled = pd.DataFrame(X_train_scaled, columns=features)\n",
    "    X_test_scaled = pd.DataFrame(X_test_scaled, columns=features)\n",
    "\n",
    "    print(\"\\nScaled Training Data Head:\")\n",
    "    print(X_train_scaled.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8734c689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# 5. Model Training and Baseline Evaluation\n",
    "# ====================================================================\n",
    "\n",
    "# Dictionary to hold the models we want to train\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(random_state=42, max_iter=1000),\n",
    "    \"K-Nearest Neighbors\": KNeighborsClassifier(),\n",
    "    \"Na√Øve Bayes\": GaussianNB(),\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=42),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(random_state=42),\n",
    "    \"MLP Classifier (Perceptron)\": MLPClassifier(random_state=42, max_iter=1000)\n",
    "}\n",
    "\n",
    "# Dictionary to store results\n",
    "results = {}\n",
    "\n",
    "if not df.empty:\n",
    "    for name, model in models.items():\n",
    "        print(f\"--- Training {name} ---\")\n",
    "        \n",
    "        # Train the model\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        \n",
    "        # Make predictions on the test set\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "        \n",
    "        # Evaluate the model\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        report = classification_report(y_test, y_pred, target_names=class_names)\n",
    "        \n",
    "        # Store results\n",
    "        results[name] = accuracy\n",
    "        \n",
    "        print(f\"\\n‚úÖ Results for {name}:\")\n",
    "        print(f\"Accuracy: {accuracy:.4f}\")\n",
    "        print(\"Classification Report:\")\n",
    "        print(report)\n",
    "        \n",
    "        # Visualize the Confusion Matrix\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(8, 6))\n",
    "        disp.plot(ax=ax, cmap='Blues', xticks_rotation='vertical')\n",
    "        ax.set_title(f'Confusion Matrix for {name}')\n",
    "        \n",
    "        # Save the figure\n",
    "        cm_path = f'reports/figures/confusion_matrix_{name.replace(\" \", \"_\")}.png'\n",
    "        plt.savefig(cm_path, bbox_inches='tight')\n",
    "        print(f\"‚úÖ Confusion matrix saved to {cm_path}\")\n",
    "        \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c960c5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# 6. Compare Model Performance\n",
    "# ====================================================================\n",
    "if results:\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    sns.barplot(x=list(results.keys()), y=list(results.values()), palette='mako')\n",
    "    plt.title('Baseline Model Comparison by Accuracy')\n",
    "    plt.xlabel('Model')\n",
    "    plt.ylabel('Accuracy Score')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.ylim(0, 1.0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    best_model_name = max(results, key=results.get)\n",
    "    print(f\"\\nüèÜ Best Performing Model (by Accuracy): {best_model_name} with an accuracy of {results[best_model_name]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057915f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# 7. Feature Importance Analysis (for best tree-based models)\n",
    "# ====================================================================\n",
    "# Let's analyze feature importance for Random Forest and Gradient Boosting\n",
    "\n",
    "tree_models = {\n",
    "    \"Random Forest\": models.get(\"Random Forest\"),\n",
    "    \"Gradient Boosting\": models.get(\"Gradient Boosting\")\n",
    "}\n",
    "\n",
    "if tree_models[\"Random Forest\"]: # Check if models were trained\n",
    "    for name, model in tree_models.items():\n",
    "        importances = model.feature_importances_\n",
    "        feature_importance_df = pd.DataFrame({'Feature': features, 'Importance': importances})\n",
    "        feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "        \n",
    "        plt.figure(figsize=(10, 5))\n",
    "        sns.barplot(x='Importance', y='Feature', data=feature_importance_df, palette='viridis')\n",
    "        plt.title(f'Feature Importance for {name}')\n",
    "        \n",
    "        # Save the figure\n",
    "        fi_path = f'reports/figures/feature_importance_{name.replace(\" \", \"_\")}.png'\n",
    "        plt.savefig(fi_path, bbox_inches='tight')\n",
    "        print(f\"‚úÖ Feature importance plot saved to {fi_path}\")\n",
    "        \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64100cad",
   "metadata": {},
   "source": [
    "### Analysis of Model Performance & Feature Importance\n",
    "\n",
    "1.  **The Top Tier: MLP, Gradient Boosting & Random Forest**\n",
    "    * **MLP Classifier (Perceptron):** Emerged as the surprising winner with the highest accuracy at **97.27%**. Its performance is outstanding across all classes, achieving a near-perfect macro average F1-score of 0.97. This indicates it was highly effective at learning the complex patterns in the data.\n",
    "    * **Gradient Boosting & Random Forest:** These two powerful ensemble models are right behind, with accuracies of **96.87%** and **96.77%**, respectively. Their classification reports show extreme robustness and ability to handle all classes well.\n",
    "\n",
    "2.  **The Strong Contender: K-Nearest Neighbors**\n",
    "    * KNN also performed exceptionally well (**96.19%** accuracy), proving to be a very capable model for this dataset with balanced F1-scores above 0.93 for all classes.\n",
    "\n",
    "3.  **The Middle Ground & Underperformer**\n",
    "    * **Na√Øve Bayes** provided a respectable accuracy of **80.61%** but struggled with some classes.\n",
    "    * **Logistic Regression** clearly failed, scoring **0.00** for precision and recall on the \"Excellent\" and \"Good\" classes, making it unsuitable for this problem.\n",
    "\n",
    "4.  **Feature Importance Insights**\n",
    "    * The feature importance plots from both Random Forest and Gradient Boosting provide a clear and consistent story.\n",
    "    * **`EC` (Electrical Conductivity) is overwhelmingly the most important feature** for predicting water quality. The Gradient Boosting model attributes over 70% of its decision-making power to this single feature.\n",
    "    * **`TDS` (Total Dissolved Solids) is the clear second most important feature.** This is logical, as EC and TDS are physically related properties.\n",
    "    * The other two features, `Cl` (Chloride) and `Na` (Sodium), have a much smaller impact, especially in the highly decisive Gradient Boosting model. This tells us that a huge amount of the predictive signal is contained within just the EC and TDS measurements.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6126a48d",
   "metadata": {},
   "source": [
    "---\n",
    "### End of Phase 2\n",
    "\n",
    "*Summary & Conclusion for Phase 2*\n",
    "\n",
    "The baseline model evaluation has been highly successful, revealing a clear distinction between model capabilities.\n",
    "\n",
    "**Key Findings:**\n",
    "\n",
    "* **Ensemble Models and Neural Networks Excel:** The tree-based ensembles (Random Forest, Gradient Boosting) and the simple neural network (MLP Classifier) proved vastly superior. Their ability to model complex, non-linear relationships allowed them to overcome the challenges of class imbalance and achieve outstanding performance (~97% accuracy).\n",
    "* **Simpler Models Struggle:** The linear model (Logistic Regression) was incapable of handling the task, while the probabilistic model (Na√Øve Bayes) showed noticeable weaknesses despite its decent overall accuracy.\n",
    "* **Feature Set is Highly Predictive:** The fact that multiple models could achieve such high accuracy using only four features (`EC`, `Cl`, `TDS`, `Na`) confirms that our feature selection in Phase 1 was highly effective.\n",
    "\n",
    "**Decision for Phase 3:**\n",
    "\n",
    "Based on these results, we have three excellent candidates for the final optimization phase: **MLP Classifier**, **Gradient Boosting**, and **Random Forest**.\n",
    "\n",
    "While the MLP Classifier had the highest accuracy, **Gradient Boosting is an ideal choice to move forward with.** It is a state-of-the-art model for tabular data, is highly tunable, and its results were nearly identical to the MLP. It represents a robust and industry-standard choice for optimization.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
