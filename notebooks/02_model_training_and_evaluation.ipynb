{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d142798f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error: 'data/raw/water_quality.csv' not found.\n"
     ]
    }
   ],
   "source": [
    "# ====================================================================\n",
    "# 0. Import Libraries and Load Data\n",
    "# ====================================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Load the dataset\n",
    "try:\n",
    "    df = pd.read_csv(\"C:/Users/Fernado/Desktop/ML_Project/data/raw/water_quality.csv\")\n",
    "    print(\"‚úÖ Dataset loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå Error: 'data/raw/water_quality.csv' not found.\")\n",
    "    df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57272b9b",
   "metadata": {},
   "source": [
    "## Phase 2: Data Preparation & Modeling\n",
    "---\n",
    "This notebook covers the data preprocessing and baseline model training steps. We will:\n",
    "1. Select the features identified in the EDA phase.\n",
    "2. Encode the target variable.\n",
    "3. Split the data into stratified training and testing sets.\n",
    "4. Apply Min-Max scaling to the features.\n",
    "5. Train and evaluate several baseline classification models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63530fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# 1. Feature and Target Selection\n",
    "# ====================================================================\n",
    "if not df.empty:\n",
    "    # Based on the correlation analysis from Phase 1\n",
    "    features = ['EC', 'Cl', 'TDS', 'Na']\n",
    "    target = 'Water Quality Classification'\n",
    "\n",
    "    X = df[features]\n",
    "    y = df[target]\n",
    "\n",
    "    print(\"Selected Features (X):\")\n",
    "    print(X.head())\n",
    "    print(\"\\nTarget Variable (y):\")\n",
    "    print(y.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d6e409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# 2. Encode the Categorical Target Variable\n",
    "# ====================================================================\n",
    "if not df.empty:\n",
    "    le = LabelEncoder()\n",
    "    y_encoded = le.fit_transform(y)\n",
    "\n",
    "    print(\"\\nOriginal Target Labels:\", le.classes_)\n",
    "    print(\"Encoded Target Labels:\", np.unique(y_encoded))\n",
    "    # Storing class names for later use in plots\n",
    "    class_names = le.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0400da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# 3. Stratified Train-Test Split\n",
    "# ====================================================================\n",
    "# We use an 80/20 split and stratify by the encoded target variable `y_encoded`\n",
    "# to ensure the class distribution is the same in both train and test sets.\n",
    "if not df.empty:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y_encoded,\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        stratify=y_encoded\n",
    "    )\n",
    "\n",
    "    print(\"Shape of training data (X_train):\", X_train.shape)\n",
    "    print(\"Shape of testing data (X_test):\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b1575e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# 4. Feature Scaling (Min-Max Scaler)\n",
    "# ====================================================================\n",
    "if not df.empty:\n",
    "    scaler = MinMaxScaler()\n",
    "\n",
    "    # Fit the scaler on the training data and transform it\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "    # Use the same fitted scaler to transform the test data\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Convert scaled arrays back to DataFrames for clarity\n",
    "    X_train_scaled = pd.DataFrame(X_train_scaled, columns=features)\n",
    "    X_test_scaled = pd.DataFrame(X_test_scaled, columns=features)\n",
    "\n",
    "    print(\"\\nScaled Training Data Head:\")\n",
    "    print(X_train_scaled.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8734c689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# 5. Model Training and Baseline Evaluation\n",
    "# ====================================================================\n",
    "\n",
    "# Dictionary to hold the models we want to train\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(random_state=42),\n",
    "    \"K-Nearest Neighbors\": KNeighborsClassifier(),\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=42),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(random_state=42)\n",
    "}\n",
    "\n",
    "# Dictionary to store results\n",
    "results = {}\n",
    "\n",
    "if not df.empty:\n",
    "    for name, model in models.items():\n",
    "        print(f\"--- Training {name} ---\")\n",
    "        \n",
    "        # Train the model\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        \n",
    "        # Make predictions on the test set\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "        \n",
    "        # Evaluate the model\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        report = classification_report(y_test, y_pred, target_names=class_names)\n",
    "        \n",
    "        # Store results\n",
    "        results[name] = accuracy\n",
    "        \n",
    "        print(f\"\\n‚úÖ Results for {name}:\")\n",
    "        print(f\"Accuracy: {accuracy:.4f}\")\n",
    "        print(\"Classification Report:\")\n",
    "        print(report)\n",
    "        \n",
    "        # Visualize the Confusion Matrix\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(8, 6))\n",
    "        disp.plot(ax=ax, cmap='Blues', xticks_rotation='vertical')\n",
    "        ax.set_title(f'Confusion Matrix for {name}')\n",
    "        \n",
    "        # Save the figure\n",
    "        cm_path = f'reports/figures/confusion_matrix_{name.replace(\" \", \"_\")}.png'\n",
    "        plt.savefig(cm_path, bbox_inches='tight')\n",
    "        print(f\"‚úÖ Confusion matrix saved to {cm_path}\")\n",
    "        \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c960c5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# 6. Compare Model Performance\n",
    "# ====================================================================\n",
    "if results:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x=list(results.keys()), y=list(results.values()), palette='mako')\n",
    "    plt.title('Baseline Model Comparison by Accuracy')\n",
    "    plt.xlabel('Model')\n",
    "    plt.ylabel('Accuracy Score')\n",
    "    plt.ylim(0, 1.0)\n",
    "    plt.show()\n",
    "\n",
    "    best_model_name = max(results, key=results.get)\n",
    "    print(f\"\\nüèÜ Best Performing Model (by Accuracy): {best_model_name} with an accuracy of {results[best_model_name]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057915f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# 7. Feature Importance Analysis (for best tree-based models)\n",
    "# ====================================================================\n",
    "# Let's analyze feature importance for Random Forest and Gradient Boosting\n",
    "\n",
    "tree_models = {\n",
    "    \"Random Forest\": models.get(\"Random Forest\"),\n",
    "    \"Gradient Boosting\": models.get(\"Gradient Boosting\")\n",
    "}\n",
    "\n",
    "if tree_models[\"Random Forest\"]: # Check if models were trained\n",
    "    for name, model in tree_models.items():\n",
    "        importances = model.feature_importances_\n",
    "        feature_importance_df = pd.DataFrame({'Feature': features, 'Importance': importances})\n",
    "        feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "        \n",
    "        plt.figure(figsize=(10, 5))\n",
    "        sns.barplot(x='Importance', y='Feature', data=feature_importance_df, palette='viridis')\n",
    "        plt.title(f'Feature Importance for {name}')\n",
    "        \n",
    "        # Save the figure\n",
    "        fi_path = f'reports/figures/feature_importance_{name.replace(\" \", \"_\")}.png'\n",
    "        plt.savefig(fi_path, bbox_inches='tight')\n",
    "        print(f\"‚úÖ Feature importance plot saved to {fi_path}\")\n",
    "        \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6126a48d",
   "metadata": {},
   "source": [
    "---\n",
    "### End of Phase 2\n",
    "*Summary & Next Steps:*\n",
    "- We have successfully preprocessed the data and trained four baseline models.\n",
    "- Based on the initial results, Random Forest and Gradient Boosting appear to be the strongest candidates.\n",
    "- The next step (Phase 3) is to take the best-performing model and use **hyperparameter tuning** (`RandomizedSearchCV`) to optimize it for even better performance.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
